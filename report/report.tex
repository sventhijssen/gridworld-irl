\documentclass[10pt,a4paper,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Sven Thijssen, Emile Valcke}
\title{Inverse Reinforcement Learning}
\begin{document}
\maketitle

\section{Introduction}
Inverse Reinforcement Learning is a learning task which 
Reinforcement learning is a learning task in which an agent tries to optimize its actions, based on its state and rewards in a given environment. However, defining a reward function can be a  in all applications an explicit reward can be given or the 

\section{Literature}
Our starting point was \textit{Inverse Reinforcement Learning} by Pieter Abbeel and Andrew Y. Ng in the \textit{Encyclopedia of Machine Learning} \cite{sammut2011encyclopedia}. According to its definition, the goal of inverse reinforcement learning is to extract a reward function, given observed behavior of an agent in an environment. The main motivation for this learning task is the difficulty of defining rewards for actions performed by the agent. 

\section{Creative part}
For the creative part of this assignment, we have opted to implement the algorithm suggested by Pieter Abbeel and Andrew Y. Ng  in the paper \textit{Apprenticeship Learning by Inverse Reinforcement Learning} \cite{abbeel2004apprenticeship}. The incentive to implement an algorithm for inverse reinforcement learning was to fully comprehend the concept. As the saying goes, ``practice makes perfect''. The initial suggestion for the creative part was inverse reinforcement learning applied to Tic-Tac-Toe. However, because of its stochastic environment, this would have led us too far for the assignment. Therefore we have opted to apply inverse reinforcement learning on a grid world such that the concepts can easily be visualized and explained.

\subsection{Implementation}
In the algorithm, multiple approaches are suggested to compute the weights $w$. One such approach would be to use a quadratic programming solver where the problem is defined as follows:

$$\max\limits_{t, w} t$$\\
s.t. $$w^T \mu_E \geq w^T \mu^{(j)} + t, j= 0,...,i-1$$
$$\|w\|_2 \leq 1$$

Another approach is to use a projection method in which the feature expectations of a policy are recalculated in each iteration

We were unable to find a suitable library to solve this problem
a reinforcement algorithm is used to compute a policy $\pi$.

%TODO q-learning
%TODO softmax
%TODO loops solution

\subsection{Experiments}

\section{Conclusion}

\bibliography{references} 
\bibliographystyle{ieeetr}



\end{document}